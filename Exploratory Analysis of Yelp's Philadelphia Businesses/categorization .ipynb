{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c70f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from numpy import std\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0af7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QdN72BWoyFypdGJhhI5r7g</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WYYdQDjx-DsCanlP0DpImQ</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O1oZpbZNDMH_gz8DhsZCdA</td>\n",
       "      <td>Burgers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dChRGpit9fM_kZK5pafNyA</td>\n",
       "      <td>Burgers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>oZzN706lKoL4faaTK739xA</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>p8FRvYD-vYAKzAiZgbJdbg</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>bmJgvpuf2GBXd62ELK0Q2w</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>gPr1io7ks0Eo3FDsnDTYfg</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>wVxXRFf10zTTAs11nr4xeA</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>951 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id  category\n",
       "0    MUTTqe8uqyMdBl186RmNeA  Japanese\n",
       "1    QdN72BWoyFypdGJhhI5r7g   Italian\n",
       "2    WYYdQDjx-DsCanlP0DpImQ  Japanese\n",
       "3    O1oZpbZNDMH_gz8DhsZCdA   Burgers\n",
       "4    dChRGpit9fM_kZK5pafNyA   Burgers\n",
       "..                      ...       ...\n",
       "946  oZzN706lKoL4faaTK739xA   Italian\n",
       "947  p8FRvYD-vYAKzAiZgbJdbg  Japanese\n",
       "948  bmJgvpuf2GBXd62ELK0Q2w   Italian\n",
       "949  gPr1io7ks0Eo3FDsnDTYfg   Italian\n",
       "950  wVxXRFf10zTTAs11nr4xeA   Italian\n",
       "\n",
       "[951 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('philly_restaurants_categories.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7239a834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiTunyQ73aT9WBnpR9DZGw</td>\n",
       "      <td>OyoGAe7OKpv6SyGZT5g77Q</td>\n",
       "      <td>7ATYjTIgM3jUlt4UM3IypQ</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>2012-01-03 15:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
       "      <td>eUta8W_HdHMXPzLBBZhL1A</td>\n",
       "      <td>04UD14gamNjLY0IDYVhHJg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>2015-09-23 23:10:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8JFGBuHMoiNDyfcxuWNtrA</td>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good food--loved the gnocchi with marinara\\nth...</td>\n",
       "      <td>2009-10-14 19:57:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oyaMhzBSwfGgemSGuZCdwQ</td>\n",
       "      <td>Dd1jQj7S-BFGqRbApFzCFw</td>\n",
       "      <td>YtSqYv1Q_pOltsVPSx54SA</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tremendous service (Big shout out to Douglas) ...</td>\n",
       "      <td>2013-06-24 11:21:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967547</th>\n",
       "      <td>tWCi4N2qCil876BI2NDAZg</td>\n",
       "      <td>5d32aJe0a3_fSl3LIVYkWQ</td>\n",
       "      <td>EmrOCT7fArKI0JI_XF6aaw</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Temple University blatantly disregards freedom...</td>\n",
       "      <td>2021-08-28 18:46:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967548</th>\n",
       "      <td>nLjbVsETpqO17RbFcqskkA</td>\n",
       "      <td>am7-gkH_PDz598oTdYSD6A</td>\n",
       "      <td>3gVSrS4kffGGZT8oXHsIcw</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>*Later Yelp* I've only been here once, but I l...</td>\n",
       "      <td>2014-11-03 14:45:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967549</th>\n",
       "      <td>99ylx-qPUSseITqBye2MpA</td>\n",
       "      <td>-AkziDwQ8hv2COTDBBUpig</td>\n",
       "      <td>aunmz06iWvo3bd6MMHEbqg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Philly has become a dangerous place with the m...</td>\n",
       "      <td>2022-01-18 03:48:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967550</th>\n",
       "      <td>KlHxcAifUF5zDKpJCBrRsw</td>\n",
       "      <td>7ziWZULyiZv2TesYNMFf4g</td>\n",
       "      <td>qQO7ErS_RAN4Vs1uX0L55Q</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ice cream! ice cream sodas, sundaes!! \\n\\nwant...</td>\n",
       "      <td>2012-10-21 04:08:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967551</th>\n",
       "      <td>cACxcUY_AIsQKkpDRXuqnw</td>\n",
       "      <td>MCzlzlOw7IGbRAKVjJBPtg</td>\n",
       "      <td>fcGexL5VH5G2Xw0tRj9uOQ</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a good pizza option - they deliver thr...</td>\n",
       "      <td>2018-03-13 13:54:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>967552 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id                 user_id  \\\n",
       "0       BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q   \n",
       "1       AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ   \n",
       "2       JrIxlS1TzJ-iCu79ul40cQ  eUta8W_HdHMXPzLBBZhL1A   \n",
       "3       8JFGBuHMoiNDyfcxuWNtrA  smOvOajNG0lS4Pq7d8g4JQ   \n",
       "4       oyaMhzBSwfGgemSGuZCdwQ  Dd1jQj7S-BFGqRbApFzCFw   \n",
       "...                        ...                     ...   \n",
       "967547  tWCi4N2qCil876BI2NDAZg  5d32aJe0a3_fSl3LIVYkWQ   \n",
       "967548  nLjbVsETpqO17RbFcqskkA  am7-gkH_PDz598oTdYSD6A   \n",
       "967549  99ylx-qPUSseITqBye2MpA  -AkziDwQ8hv2COTDBBUpig   \n",
       "967550  KlHxcAifUF5zDKpJCBrRsw  7ziWZULyiZv2TesYNMFf4g   \n",
       "967551  cACxcUY_AIsQKkpDRXuqnw  MCzlzlOw7IGbRAKVjJBPtg   \n",
       "\n",
       "                   business_id  stars  useful  funny  cool  \\\n",
       "0       7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1   \n",
       "1       kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1   \n",
       "2       04UD14gamNjLY0IDYVhHJg      1       1      2     1   \n",
       "3       RZtGWDLCAtuipwaZ-UfjmQ      4       0      0     0   \n",
       "4       YtSqYv1Q_pOltsVPSx54SA      5       0      0     0   \n",
       "...                        ...    ...     ...    ...   ...   \n",
       "967547  EmrOCT7fArKI0JI_XF6aaw      1       5      2     0   \n",
       "967548  3gVSrS4kffGGZT8oXHsIcw      3       2      0     2   \n",
       "967549  aunmz06iWvo3bd6MMHEbqg      3       0      0     0   \n",
       "967550  qQO7ErS_RAN4Vs1uX0L55Q      4       1      0     0   \n",
       "967551  fcGexL5VH5G2Xw0tRj9uOQ      3       1      1     0   \n",
       "\n",
       "                                                     text                 date  \n",
       "0       I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n",
       "1       Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
       "2       I am a long term frequent customer of this est...  2015-09-23 23:10:31  \n",
       "3       Good food--loved the gnocchi with marinara\\nth...  2009-10-14 19:57:14  \n",
       "4       Tremendous service (Big shout out to Douglas) ...  2013-06-24 11:21:25  \n",
       "...                                                   ...                  ...  \n",
       "967547  Temple University blatantly disregards freedom...  2021-08-28 18:46:05  \n",
       "967548  *Later Yelp* I've only been here once, but I l...  2014-11-03 14:45:46  \n",
       "967549  Philly has become a dangerous place with the m...  2022-01-18 03:48:44  \n",
       "967550  ice cream! ice cream sodas, sundaes!! \\n\\nwant...  2012-10-21 04:08:40  \n",
       "967551  This is a good pizza option - they deliver thr...  2018-03-13 13:54:48  \n",
       "\n",
       "[967552 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('yelp_academic_dataset_reviews.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa0a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df[['business_id', 'category']].merge(df1[['business_id', 'text']], on='business_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883ea24a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Stopped in to check out this new spot around t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>I live in the neighborhood and used to order a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>We came here tonight just for a date night. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Wow! What a great dining adventure! Huge rolls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Wonderfully fresh sushi, amazing lobster fried...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155675</th>\n",
       "      <td>wVxXRFf10zTTAs11nr4xeA</td>\n",
       "      <td>Italian</td>\n",
       "      <td>I was online and couldn't decide what to order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155676</th>\n",
       "      <td>wVxXRFf10zTTAs11nr4xeA</td>\n",
       "      <td>Italian</td>\n",
       "      <td>My partner raved about Primo Hoagies so I was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155677</th>\n",
       "      <td>wVxXRFf10zTTAs11nr4xeA</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Well built hoagie, nice and full...bread, meat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155678</th>\n",
       "      <td>wVxXRFf10zTTAs11nr4xeA</td>\n",
       "      <td>Italian</td>\n",
       "      <td>My boyfriend (vegetarian) and I ordered from P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155679</th>\n",
       "      <td>wVxXRFf10zTTAs11nr4xeA</td>\n",
       "      <td>Italian</td>\n",
       "      <td>For a place that only makes hoagies how do u f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   business_id  category  \\\n",
       "0       MUTTqe8uqyMdBl186RmNeA  Japanese   \n",
       "1       MUTTqe8uqyMdBl186RmNeA  Japanese   \n",
       "2       MUTTqe8uqyMdBl186RmNeA  Japanese   \n",
       "3       MUTTqe8uqyMdBl186RmNeA  Japanese   \n",
       "4       MUTTqe8uqyMdBl186RmNeA  Japanese   \n",
       "...                        ...       ...   \n",
       "155675  wVxXRFf10zTTAs11nr4xeA   Italian   \n",
       "155676  wVxXRFf10zTTAs11nr4xeA   Italian   \n",
       "155677  wVxXRFf10zTTAs11nr4xeA   Italian   \n",
       "155678  wVxXRFf10zTTAs11nr4xeA   Italian   \n",
       "155679  wVxXRFf10zTTAs11nr4xeA   Italian   \n",
       "\n",
       "                                                     text  \n",
       "0       Stopped in to check out this new spot around t...  \n",
       "1       I live in the neighborhood and used to order a...  \n",
       "2       We came here tonight just for a date night. We...  \n",
       "3       Wow! What a great dining adventure! Huge rolls...  \n",
       "4       Wonderfully fresh sushi, amazing lobster fried...  \n",
       "...                                                   ...  \n",
       "155675  I was online and couldn't decide what to order...  \n",
       "155676  My partner raved about Primo Hoagies so I was ...  \n",
       "155677  Well built hoagie, nice and full...bread, meat...  \n",
       "155678  My boyfriend (vegetarian) and I ordered from P...  \n",
       "155679  For a place that only makes hoagies how do u f...  \n",
       "\n",
       "[155680 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d23aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = merged_df.groupby('business_id')['text'].agg(' '.join).reset_index()\n",
    "merged_df = grouped_df.merge(merged_df[['business_id', 'category']].drop_duplicates(), on='business_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46519e8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X= merged_df['text'].tolist()\n",
    "y= merged_df['category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a575a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Italian',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Italian',\n",
       " 'Burgers',\n",
       " 'Japanese',\n",
       " 'Italian']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1b7dd",
   "metadata": {},
   "source": [
    "                                                -Step 1-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf6b79c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with highest positive weights:\n",
      "           feature  coefficient\n",
      "9182        burger     2.509751\n",
      "23549        fries     2.159326\n",
      "9206       burgers     1.214216\n",
      "35483     mcdonald     1.053822\n",
      "18474        drive     0.938639\n",
      "8381     breakfast     0.855071\n",
      "62990        wendy     0.851034\n",
      "11363  cheesesteak     0.823566\n",
      "63499        wings     0.764589\n",
      "49677     sandwich     0.753226\n",
      "40199        order     0.752353\n",
      "22825         food     0.688003\n",
      "11335       cheese     0.629316\n",
      "11274     checkers     0.616852\n",
      "34309          mac     0.616627\n",
      "28751         ihop     0.605009\n",
      "35484    mcdonalds     0.520513\n",
      "5585         bacon     0.502462\n",
      "41265     pancakes     0.479658\n",
      "63459       window     0.474651\n",
      "Words with lowest negative weights:\n",
      "          feature  coefficient\n",
      "17471      dishes    -0.470386\n",
      "53611        soup    -0.473634\n",
      "35614   meatballs    -0.478196\n",
      "28492         hut    -0.493847\n",
      "30666    japanese    -0.497871\n",
      "9466         byob    -0.502768\n",
      "16266   delicious    -0.514812\n",
      "17204      dinner    -0.559798\n",
      "11692     chinese    -0.567209\n",
      "49911       sauce    -0.569391\n",
      "47800  restaurant    -0.585329\n",
      "44031        pork    -0.622190\n",
      "32130      korean    -0.624991\n",
      "63470        wine    -0.689957\n",
      "48188        rice    -0.759943\n",
      "46169       ramen    -0.984504\n",
      "43286       pizza    -0.989202\n",
      "41728       pasta    -1.149181\n",
      "30389     italian    -1.572122\n",
      "56057       sushi    -1.693115\n",
      "Avarage Accuracy for Logistic Regression: 92.22%\n",
      "Avarage Precision for Logistic Regression: 93.30%\n",
      "Avarage Recall for Logistic Regression: 92.10%\n",
      "Avarage F1-measure for Logistic Regression: 92.64%\n",
      "The avarage confusion matrix for Logistic Regression is:\n",
      "[[56.2  6.8  0.2]\n",
      " [ 4.8 81.   0.4]\n",
      " [ 0.8  1.8 38.2]]\n",
      "Avarage Accuracy for SVM: 92.11%\n",
      "Avarage Precision for SVM: 93.11%\n",
      "Avarage Recall for SVM: 92.38%\n",
      "Avarage F1-measure for SVM: 92.71%\n",
      "The avarage confusion matrix for SVM is:\n",
      "[[55.6  7.4  0.2]\n",
      " [ 5.4 80.4  0.4]\n",
      " [ 0.6  1.  39.2]]\n",
      "Avarage Accuracy for K-NN: 92.11%\n",
      "Avarage Precision for K-NN: 90.73%\n",
      "Avarage Recall for K-NN: 89.59%\n",
      "Avarage F1-measure K-NN: SVM 90.07%\n",
      "The avarage confusion matrix for K-NN is:\n",
      "[[52.6 10.2  0.4]\n",
      " [ 7.2 78.6  0.4]\n",
      " [ 1.   1.2 38.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "metrics_avg_logreg = np.zeros(4)\n",
    "metrics_avg_svm = np.zeros(4)\n",
    "metrics_avg_kNN = np.zeros(4)\n",
    "avg_cm_logreg = None\n",
    "avg_cm_svm = None\n",
    "avg_cm_kNN= None\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "#Japanese:2 , Italian:1 , Burgers:0\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_count = 0\n",
    "# split the data into 5 folds\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    fold_count += 1\n",
    "    X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    #tf-idf vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    " \n",
    "    #logistic Regression\n",
    "    logreg =LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "    logreg.fit(X_train_vectorized, y_train)\n",
    "    logreg_pred = logreg.predict(X_test_vectorized)\n",
    "    logreg_precision, logreg_recall, logreg_fscore, logreg_support = precision_recall_fscore_support(y_test, logreg_pred, average='macro')\n",
    "    logreg_acc = logreg.score(X_test_vectorized, y_test)\n",
    "    logreg_f1score_score_last3 = f1_score(y_test, logreg_pred, average='macro', labels=[0, 1, 2])\n",
    "    metrics_avg_logreg[0] += logreg_acc\n",
    "    metrics_avg_logreg[1] += logreg_precision\n",
    "    metrics_avg_logreg[2] += logreg_recall\n",
    "    metrics_avg_logreg[3] += logreg_f1score_score_last3\n",
    "    cm_logreg = confusion_matrix(y_test, logreg_pred)\n",
    "    if avg_cm_logreg is None:\n",
    "        avg_cm_logreg = cm_logreg\n",
    "    else:\n",
    "        avg_cm_logreg += cm_logreg if cm_logreg is not None else cm_logreg\n",
    "    \n",
    "    #SVM classifier\n",
    "    svm = SVC(random_state=42)\n",
    "    svm.fit(X_train_vectorized, y_train)\n",
    "    svm_pred = svm.predict(X_test_vectorized)\n",
    "    svm_precision, svm_recall, svm_fscore, svm_support = precision_recall_fscore_support(y_test, svm_pred, average='macro')\n",
    "    svm_acc = svm.score(X_test_vectorized, y_test)\n",
    "    svm_f1score_score_last3 = f1_score(y_test, svm_pred, average='macro', labels=[0, 1, 2])\n",
    "    metrics_avg_svm[0] += svm_acc\n",
    "    metrics_avg_svm[1] += svm_precision\n",
    "    metrics_avg_svm[2] += svm_recall\n",
    "    metrics_avg_svm[3] += svm_f1score_score_last3\n",
    "    cm_svm = confusion_matrix(y_test, svm_pred)\n",
    "    if avg_cm_svm is None:\n",
    "        avg_cm_svm = cm_svm\n",
    "    else:\n",
    "        avg_cm_svm += cm_svm\n",
    "    \n",
    "    #K-NN classifier\n",
    "    kNN = KNeighborsClassifier()\n",
    "    kNN.fit(X_train_vectorized, y_train)\n",
    "    kNN_pred = kNN.predict(X_test_vectorized)\n",
    "    kNN_precision, kNN_recall, kNN_fscore, kNN_support = precision_recall_fscore_support(y_test, kNN_pred, average='macro')\n",
    "    kNN_acc = svm.score(X_test_vectorized, y_test)\n",
    "    kNN_f1score_score_last3 = f1_score(y_test,kNN_pred , average='macro', labels=[0, 1, 2])\n",
    "    metrics_avg_kNN[0] += kNN_acc\n",
    "    metrics_avg_kNN[1] += kNN_precision\n",
    "    metrics_avg_kNN[2] += kNN_recall\n",
    "    metrics_avg_kNN[3] += kNN_f1score_score_last3\n",
    "    cm_kNN = confusion_matrix(y_test, kNN_pred)\n",
    "    if avg_cm_kNN is None:\n",
    "        avg_cm_kNN = cm_kNN\n",
    "    else:\n",
    "        avg_cm_kNN += cm_kNN\n",
    "    \n",
    "    if fold_count == 5:\n",
    "\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        coefficients = logreg.coef_[0]\n",
    "        coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n",
    "        coef_df.sort_values(by='coefficient', ascending=False, inplace=True)\n",
    "        print(\"Words with highest positive weights:\")\n",
    "        print(coef_df.head(20))\n",
    "        print(\"Words with lowest negative weights:\")\n",
    "        print(coef_df.tail(20))\n",
    "        \n",
    "#All the prints        \n",
    "print(\"Avarage Accuracy for Logistic Regression: {:.2f}%\".format(100 * metrics_avg_logreg[0]/5))\n",
    "print(\"Avarage Precision for Logistic Regression: {:.2f}%\".format(100 * metrics_avg_logreg[1]/5))\n",
    "print(\"Avarage Recall for Logistic Regression: {:.2f}%\".format(100 * metrics_avg_logreg[2]/5))\n",
    "print(\"Avarage F1-measure for Logistic Regression: {:.2f}%\".format(100 * metrics_avg_logreg[3]/5))\n",
    "print(\"The avarage confusion matrix for Logistic Regression is:\")\n",
    "print(avg_cm_logreg/5)\n",
    "\n",
    "print(\"Avarage Accuracy for SVM: {:.2f}%\".format(100 * metrics_avg_svm[0]/5))\n",
    "print(\"Avarage Precision for SVM: {:.2f}%\".format(100 * metrics_avg_svm[1]/5))\n",
    "print(\"Avarage Recall for SVM: {:.2f}%\".format(100 * metrics_avg_svm[2]/5))\n",
    "print(\"Avarage F1-measure for SVM: {:.2f}%\".format(100 * metrics_avg_svm[3]/5))\n",
    "print(\"The avarage confusion matrix for SVM is:\")\n",
    "print(avg_cm_svm/5)\n",
    "\n",
    "print(\"Avarage Accuracy for K-NN: {:.2f}%\".format(100 * metrics_avg_kNN[0]/5))\n",
    "print(\"Avarage Precision for K-NN: {:.2f}%\".format(100 * metrics_avg_kNN[1]/5))\n",
    "print(\"Avarage Recall for K-NN: {:.2f}%\".format(100 * metrics_avg_kNN[2]/5))\n",
    "print(\"Avarage F1-measure K-NN: SVM {:.2f}%\".format(100 * metrics_avg_kNN[3]/5))\n",
    "print(\"The avarage confusion matrix for K-NN is:\")\n",
    "print(avg_cm_kNN/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4229b5",
   "metadata": {},
   "source": [
    "Based on the features with highest positive weights and the features with lowest negative weights The features with the highest positive weights suggest that the classifier is associating these features with the \"Burgers\" category, as all of these words are related to fast food and American cuisine.On the other hand, the features with the lowest negative weights suggest that the classifier is associating these features with the \"Japanese\" and \"Italian\" categories.From this, we can conclude that the classifier is able to differentiate between the three categories to some degree, but there is room for improvement in terms of reducing the overlap between the \"Burgers\" category  with the  \"Japanese\" or \"Italian\" categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de48ed",
   "metadata": {},
   "source": [
    "                               -Step 2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e3bdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gensim \n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "word_by_word_X = [utils.simple_preprocess(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ed7cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cbow_metrics_avg_logreg = np.zeros(4)\n",
    "cbow_metrics_avg_svm = np.zeros(4)\n",
    "cbow_metrics_avg_kNN = np.zeros(4)\n",
    "cbow_avg_cm_logreg = None\n",
    "cbow_avg_cm_svm = None\n",
    "cbow_avg_cm_kNN= None\n",
    "\n",
    "doc2vec_metrics_avg_logreg = np.zeros(4)\n",
    "doc2vec_metrics_avg_svm = np.zeros(4)\n",
    "doc2vec_metrics_avg_kNN = np.zeros(4)\n",
    "doc2vec_avg_cm_logreg = None\n",
    "doc2vec_avg_cm_svm = None\n",
    "doc2vec_avg_cm_kNN= None\n",
    "\n",
    "skip_gram_metrics_avg_logreg = np.zeros(4)\n",
    "skip_gram_metrics_avg_svm = np.zeros(4)\n",
    "skip_gram_metrics_avg_kNN = np.zeros(4)\n",
    "skip_gram_avg_cm_logreg = None\n",
    "skip_gram_avg_cm_svm = None\n",
    "skip_gram_avg_cm_kNN= None\n",
    "\n",
    "# Split the data into 5 folds\n",
    "for train_index, test_index in kfold.split(word_by_word_X):\n",
    "    X_train, X_test = [word_by_word_X[i] for i in train_index], [word_by_word_X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "   \n",
    "    #Chow Model \n",
    "    cbow_model = Word2Vec(sentences=X_train, vector_size=50, window=10, min_count=1, workers=4)    \n",
    "    cbow_model.train(X_train, total_examples=len(X_train), epochs=10)\n",
    "    X_avg_train_cbow = [np.array([cbow_model.wv[x] for x in y]).mean(axis = 0) for y in X_train]\n",
    "    X_avg_test_cbow = [np.array([cbow_model.wv[x] for x in y if x in cbow_model.wv]).mean(axis = 0) for y in X_test]\n",
    "    #SkipGram Model \n",
    "    skip_gram_model = Word2Vec(sentences=X_train, vector_size=50, window=10, min_count=1, workers=4, sg=1)\n",
    "    skip_gram_model.train(X_train, total_examples=len(X_train), epochs=10)\n",
    "    X_avg_train_skip_gram = [np.array([skip_gram_model.wv[x] for x in y]).mean(axis = 0) for y in X_train]\n",
    "    X_avg_test_skip_gram = [np.array([skip_gram_model.wv[x] for x in y if x in skip_gram_model.wv]).mean(axis = 0) for y in X_test]\n",
    "\n",
    "    \n",
    "    \n",
    "    #Doc2Vec model\n",
    "    tagged_X_train = [gensim.models.doc2vec.TaggedDocument(X_train[i], [i]) for i in range(len(X_train))]\n",
    "    d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    d2v_model.build_vocab(tagged_X_train)\n",
    "    d2v_model.train(tagged_X_train, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "    X_train_d2v = [d2v_model.infer_vector(x) for x in X_train]\n",
    "    X_test_d2v = [d2v_model.infer_vector(x) for x in X_test]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #classifiers for chow model\n",
    "    lr_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "    lr_clf.fit(np.array(X_avg_train_cbow), np.array(y_train))\n",
    "    lr_clf_pred = lr_clf.predict(X_avg_test_cbow)\n",
    "    lr_clf_precision = precision_score(y_test, lr_clf_pred, average='macro') \n",
    "    lr_clf_f1 = f1_score(y_test, lr_clf_pred, average='macro', labels=[0, 1, 2])\n",
    "    lr_clf_accuracy = accuracy_score(y_test, lr_clf_pred)\n",
    "    lr_clf_recall = recall_score(y_test, lr_clf_pred, average='macro')\n",
    "\n",
    "    cbow_metrics_avg_logreg[0] += lr_clf_accuracy\n",
    "    cbow_metrics_avg_logreg[1] += lr_clf_precision\n",
    "    cbow_metrics_avg_logreg[2] += lr_clf_recall\n",
    "    cbow_metrics_avg_logreg[3] += lr_clf_f1\n",
    "    cbow_cm_logreg = confusion_matrix(y_test, lr_clf_pred)\n",
    "    if cbow_avg_cm_logreg is None:\n",
    "        cbow_avg_cm_logreg = cbow_cm_logreg\n",
    "    else:\n",
    "        cbow_avg_cm_logreg += cbow_cm_logreg \n",
    "        \n",
    "    svm = SVC(random_state=42)\n",
    "    svm.fit(X_avg_train_cbow, y_train)\n",
    "    svm_pred = svm.predict(X_avg_test_cbow)\n",
    "    svm_precision = precision_score(y_test, svm_pred, average='macro') \n",
    "    svm_f1 = f1_score(y_test, svm_pred, average='macro', labels=[0, 1, 2])\n",
    "    svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "    svm_recall = recall_score(y_test, svm_pred, average='macro')\n",
    "\n",
    "    cbow_metrics_avg_svm[0] += svm_accuracy\n",
    "    cbow_metrics_avg_svm[1] += svm_precision\n",
    "    cbow_metrics_avg_svm[2] += svm_recall\n",
    "    cbow_metrics_avg_svm[3] += svm_f1\n",
    "    cbow_cm_svm = confusion_matrix(y_test, svm_pred)\n",
    "    if cbow_avg_cm_svm is None:\n",
    "        cbow_avg_cm_svm = cbow_cm_svm\n",
    "    else:\n",
    "        cbow_avg_cm_svm += cbow_cm_svm\n",
    "    \n",
    "\n",
    "    kNN = KNeighborsClassifier()\n",
    "    kNN.fit(np.array(X_avg_train_cbow), np.array(y_train))\n",
    "    kNN_pred = lr_clf.predict(X_avg_test_cbow)\n",
    "    kNN_precision = precision_score(y_test, kNN_pred, average='macro') \n",
    "    kNN_f1 = f1_score(y_test, kNN_pred, average='macro', labels=[0, 1, 2])\n",
    "    kNN_accuracy = accuracy_score(y_test, kNN_pred)\n",
    "    kNN_recall = recall_score(y_test, kNN_pred, average='macro')\n",
    "\n",
    "    cbow_metrics_avg_kNN[0] += kNN_accuracy\n",
    "    cbow_metrics_avg_kNN[1] += kNN_precision\n",
    "    cbow_metrics_avg_kNN[2] += kNN_recall\n",
    "    cbow_metrics_avg_kNN[3] += kNN_f1\n",
    "    cbow_cm_kNN = confusion_matrix(y_test, kNN_pred)\n",
    "    if cbow_avg_cm_kNN is None:\n",
    "        cbow_avg_cm_kNN = cbow_cm_kNN\n",
    "    else:\n",
    "        cbow_avg_cm_kNN += cbow_cm_kNN\n",
    "\n",
    "        \n",
    "    #classifiers for skipgram model\n",
    "    lr_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "    lr_clf.fit(np.array(X_avg_train_skip_gram), np.array(y_train))\n",
    "    lr_clf_pred = lr_clf.predict(X_avg_test_skip_gram)\n",
    "    lr_clf_precision = precision_score(y_test, lr_clf_pred, average='macro') \n",
    "    lr_clf_f1 = f1_score(y_test, lr_clf_pred, average='macro', labels=[0, 1, 2])\n",
    "    lr_clf_accuracy = accuracy_score(y_test, lr_clf_pred)\n",
    "    lr_clf_recall = recall_score(y_test, lr_clf_pred, average='macro')\n",
    "\n",
    "    skip_gram_metrics_avg_logreg[0] += lr_clf_accuracy\n",
    "    skip_gram_metrics_avg_logreg[1] += lr_clf_precision\n",
    "    skip_gram_metrics_avg_logreg[2] += lr_clf_recall\n",
    "    skip_gram_metrics_avg_logreg[3] += lr_clf_f1\n",
    "    skip_gram_cm_logreg = confusion_matrix(y_test, lr_clf_pred)\n",
    "    if skip_gram_avg_cm_logreg is None:\n",
    "        skip_gram_avg_cm_logreg = skip_gram_cm_logreg\n",
    "    else:\n",
    "        skip_gram_avg_cm_logreg += skip_gram_cm_logreg \n",
    "        \n",
    "    svm = SVC(random_state=42)\n",
    "    svm.fit(X_avg_train_skip_gram, y_train)\n",
    "    svm_pred = svm.predict(X_avg_test_skip_gram)\n",
    "    svm_precision = precision_score(y_test, svm_pred, average='macro') \n",
    "    svm_f1 = f1_score(y_test, svm_pred, average='macro', labels=[0, 1, 2])\n",
    "    svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "    svm_recall = recall_score(y_test, svm_pred, average='macro')\n",
    "\n",
    "    skip_gram_metrics_avg_svm[0] += svm_accuracy\n",
    "    skip_gram_metrics_avg_svm[1] += svm_precision\n",
    "    skip_gram_metrics_avg_svm[2] += svm_recall\n",
    "    skip_gram_metrics_avg_svm[3] += svm_f1\n",
    "    skip_gram_cm_svm = confusion_matrix(y_test, svm_pred)\n",
    "    if skip_gram_avg_cm_svm is None:\n",
    "        skip_gram_avg_cm_svm = skip_gram_cm_svm\n",
    "    else:\n",
    "        skip_gram_avg_cm_svm += skip_gram_cm_svm\n",
    "    \n",
    "\n",
    "    kNN = KNeighborsClassifier()\n",
    "    kNN.fit(np.array(X_avg_train_skip_gram), np.array(y_train))\n",
    "    kNN_pred = lr_clf.predict(X_avg_test_skip_gram)\n",
    "    kNN_precision = precision_score(y_test, kNN_pred, average='macro') \n",
    "    kNN_f1 = f1_score(y_test, kNN_pred, average='macro', labels=[0, 1, 2])\n",
    "    kNN_accuracy = accuracy_score(y_test, kNN_pred)\n",
    "    kNN_recall = recall_score(y_test, kNN_pred, average='macro')\n",
    "\n",
    "    skip_gram_metrics_avg_kNN[0] += kNN_accuracy\n",
    "    skip_gram_metrics_avg_kNN[1] += kNN_precision\n",
    "    skip_gram_metrics_avg_kNN[2] += kNN_recall\n",
    "    skip_gram_metrics_avg_kNN[3] += kNN_f1\n",
    "    skip_gram_cm_kNN = confusion_matrix(y_test, kNN_pred)\n",
    "    if skip_gram_avg_cm_kNN is None:\n",
    "        skip_gram_avg_cm_kNN = skip_gram_cm_kNN\n",
    "    else:\n",
    "        skip_gram_avg_cm_kNN += skip_gram_cm_kNN\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    #classifiers for Doc2Vec model\n",
    "    lr_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "    lr_clf.fit(np.array(X_train_d2v), np.array(y_train))\n",
    "    lr_clf_pred = lr_clf.predict(X_test_d2v)\n",
    "    lr_clf_precision = precision_score(y_test, lr_clf_pred, average='macro') \n",
    "    lr_clf_f1 = f1_score(y_test, lr_clf_pred, average='macro', labels=[0, 1, 2])\n",
    "    lr_clf_accuracy = accuracy_score(y_test, lr_clf_pred)\n",
    "    lr_clf_recall = recall_score(y_test, lr_clf_pred, average='macro')\n",
    "\n",
    "    doc2vec_metrics_avg_logreg[0] += lr_clf_accuracy\n",
    "    doc2vec_metrics_avg_logreg[1] += lr_clf_precision\n",
    "    doc2vec_metrics_avg_logreg[2] += lr_clf_recall\n",
    "    doc2vec_metrics_avg_logreg[3] += lr_clf_f1\n",
    "    doc2vec_cm_logreg = confusion_matrix(y_test, lr_clf_pred)\n",
    "    if doc2vec_avg_cm_logreg is None:\n",
    "        doc2vec_avg_cm_logreg = doc2vec_cm_logreg\n",
    "    else:\n",
    "        doc2vec_avg_cm_logreg += doc2vec_cm_logreg \n",
    "        \n",
    "    svm = SVC(random_state=42)\n",
    "    svm.fit(X_train_d2v, y_train)\n",
    "    svm_pred = svm.predict(X_test_d2v)\n",
    "    svm_precision = precision_score(y_test, svm_pred, average='macro') \n",
    "    svm_f1 = f1_score(y_test, svm_pred, average='macro', labels=[0, 1, 2])\n",
    "    svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "    svm_recall = recall_score(y_test, svm_pred, average='macro')\n",
    "\n",
    "    doc2vec_metrics_avg_svm[0] += svm_accuracy\n",
    "    doc2vec_metrics_avg_svm[1] += svm_precision\n",
    "    doc2vec_metrics_avg_svm[2] += svm_recall\n",
    "    doc2vec_metrics_avg_svm[3] += svm_f1\n",
    "    doc2vec_cm_svm = confusion_matrix(y_test, svm_pred)\n",
    "    if doc2vec_avg_cm_svm is None:\n",
    "        doc2vec_avg_cm_svm = doc2vec_cm_svm\n",
    "    else:\n",
    "        doc2vec_avg_cm_svm += doc2vec_cm_svm\n",
    "    \n",
    "\n",
    "    kNN = KNeighborsClassifier()\n",
    "    kNN.fit(np.array(X_train_d2v), np.array(y_train))\n",
    "    kNN_pred = lr_clf.predict(X_test_d2v)\n",
    "    kNN_precision = precision_score(y_test, kNN_pred, average='macro') \n",
    "    kNN_f1 = f1_score(y_test, kNN_pred, average='macro', labels=[0, 1, 2])\n",
    "    kNN_accuracy = accuracy_score(y_test, kNN_pred)\n",
    "    kNN_recall = recall_score(y_test, kNN_pred, average='macro')\n",
    "\n",
    "    doc2vec_metrics_avg_kNN[0] += kNN_accuracy\n",
    "    doc2vec_metrics_avg_kNN[1] += kNN_precision\n",
    "    doc2vec_metrics_avg_kNN[2] += kNN_recall\n",
    "    doc2vec_metrics_avg_kNN[3] += kNN_f1\n",
    "    doc2vec_cm_kNN = confusion_matrix(y_test, kNN_pred)\n",
    "    if doc2vec_avg_cm_kNN is None:\n",
    "        doc2vec_avg_cm_kNN = doc2vec_cm_kNN\n",
    "    else:\n",
    "        doc2vec_avg_cm_kNN += doc2vec_cm_kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6d8191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For CBOW model: \n",
      "Avarage Accuracy for Logistic Regression: 91.80%\n",
      "Avarage Precision for Logistic Regression: 92.72%\n",
      "Avarage Recall for Logistic Regression: 92.40%\n",
      "Avarage F1-measure for Logistic Regression: 92.52%\n",
      "The avarage confusion matrix for Logistic Regression is:\n",
      "[[56.2  6.6  0.4]\n",
      " [ 7.  78.8  0.4]\n",
      " [ 0.2  1.  39.6]]\n",
      "Avarage Accuracy for SVM: 91.38%\n",
      "Avarage Precision for SVM: 92.33%\n",
      "Avarage Recall for SVM: 91.65%\n",
      "Avarage F1-measure for SVM: 91.95%\n",
      "The avarage confusion matrix for SVM is:\n",
      "[[55.4  7.2  0.6]\n",
      " [ 6.2 79.6  0.4]\n",
      " [ 1.   1.  38.8]]\n",
      "Avarage Accuracy for K-NN: 91.80%\n",
      "Avarage Precision for K-NN: 92.72%\n",
      "Avarage Recall for K-NN: 92.40%\n",
      "Avarage F1-measure K-NN: SVM 92.52%\n",
      "The avarage confusion matrix for K-NN is:\n",
      "[[56.2  6.6  0.4]\n",
      " [ 7.  78.8  0.4]\n",
      " [ 0.2  1.  39.6]]\n"
     ]
    }
   ],
   "source": [
    "print(\"For CBOW model: \")\n",
    "print(\"Avarage Accuracy for Logistic Regression: {:.2f}%\".format(100 * cbow_metrics_avg_logreg[0]/5))\n",
    "print(\"Avarage Precision for Logistic Regression: {:.2f}%\".format(100 * cbow_metrics_avg_logreg[1]/5))\n",
    "print(\"Avarage Recall for Logistic Regression: {:.2f}%\".format(100 * cbow_metrics_avg_logreg[2]/5))\n",
    "print(\"Avarage F1-measure for Logistic Regression: {:.2f}%\".format(100 * cbow_metrics_avg_logreg[3]/5))\n",
    "print(\"The avarage confusion matrix for Logistic Regression is:\")\n",
    "print(cbow_avg_cm_logreg/5)\n",
    "\n",
    "print(\"Avarage Accuracy for SVM: {:.2f}%\".format(100 * cbow_metrics_avg_svm[0]/5))\n",
    "print(\"Avarage Precision for SVM: {:.2f}%\".format(100 * cbow_metrics_avg_svm[1]/5))\n",
    "print(\"Avarage Recall for SVM: {:.2f}%\".format(100 * cbow_metrics_avg_svm[2]/5))\n",
    "print(\"Avarage F1-measure for SVM: {:.2f}%\".format(100 * cbow_metrics_avg_svm[3]/5))\n",
    "print(\"The avarage confusion matrix for SVM is:\")\n",
    "print(cbow_avg_cm_svm/5)\n",
    "\n",
    "print(\"Avarage Accuracy for K-NN: {:.2f}%\".format(100 * cbow_metrics_avg_kNN[0]/5))\n",
    "print(\"Avarage Precision for K-NN: {:.2f}%\".format(100 * cbow_metrics_avg_kNN[1]/5))\n",
    "print(\"Avarage Recall for K-NN: {:.2f}%\".format(100 * cbow_metrics_avg_kNN[2]/5))\n",
    "print(\"Avarage F1-measure K-NN: SVM {:.2f}%\".format(100 * cbow_metrics_avg_kNN[3]/5))\n",
    "print(\"The avarage confusion matrix for K-NN is:\")\n",
    "print(cbow_avg_cm_kNN/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c17cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Skipgram model: \n",
      "Avarage Accuracy for Logistic Regression: 86.33%\n",
      "Avarage Precision for Logistic Regression: 88.92%\n",
      "Avarage Recall for Logistic Regression: 86.01%\n",
      "Avarage F1-measure for Logistic Regression: 87.16%\n",
      "The avarage confusion matrix for Logistic Regression is:\n",
      "[[48.6 14.4  0.2]\n",
      " [ 6.6 79.2  0.4]\n",
      " [ 0.6  3.8 36.4]]\n",
      "Avarage Accuracy for SVM: 89.59%\n",
      "Avarage Precision for SVM: 91.78%\n",
      "Avarage Recall for SVM: 88.86%\n",
      "Avarage F1-measure for SVM: 90.00%\n",
      "The avarage confusion matrix for SVM is:\n",
      "[[51.  12.   0.2]\n",
      " [ 3.2 82.6  0.4]\n",
      " [ 1.   3.  36.8]]\n",
      "Avarage Accuracy for K-NN: 86.33%\n",
      "Avarage Precision for K-NN: 88.92%\n",
      "Avarage Recall for K-NN: 86.01%\n",
      "Avarage F1-measure K-NN: SVM 87.16%\n",
      "The avarage confusion matrix for K-NN is:\n",
      "[[48.6 14.4  0.2]\n",
      " [ 6.6 79.2  0.4]\n",
      " [ 0.6  3.8 36.4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"For Skipgram model: \")\n",
    "print(\"Avarage Accuracy for Logistic Regression: {:.2f}%\".format(100 * skip_gram_metrics_avg_logreg[0]/5))\n",
    "print(\"Avarage Precision for Logistic Regression: {:.2f}%\".format(100 * skip_gram_metrics_avg_logreg[1]/5))\n",
    "print(\"Avarage Recall for Logistic Regression: {:.2f}%\".format(100 * skip_gram_metrics_avg_logreg[2]/5))\n",
    "print(\"Avarage F1-measure for Logistic Regression: {:.2f}%\".format(100 * skip_gram_metrics_avg_logreg[3]/5))\n",
    "print(\"The avarage confusion matrix for Logistic Regression is:\")\n",
    "print(skip_gram_avg_cm_logreg/5)\n",
    "\n",
    "print(\"Avarage Accuracy for SVM: {:.2f}%\".format(100 * skip_gram_metrics_avg_svm[0]/5))\n",
    "print(\"Avarage Precision for SVM: {:.2f}%\".format(100 * skip_gram_metrics_avg_svm[1]/5))\n",
    "print(\"Avarage Recall for SVM: {:.2f}%\".format(100 * skip_gram_metrics_avg_svm[2]/5))\n",
    "print(\"Avarage F1-measure for SVM: {:.2f}%\".format(100 * skip_gram_metrics_avg_svm[3]/5))\n",
    "print(\"The avarage confusion matrix for SVM is:\")\n",
    "print(skip_gram_avg_cm_svm/5)\n",
    "\n",
    "print(\"Avarage Accuracy for K-NN: {:.2f}%\".format(100 * skip_gram_metrics_avg_kNN[0]/5))\n",
    "print(\"Avarage Precision for K-NN: {:.2f}%\".format(100 * skip_gram_metrics_avg_kNN[1]/5))\n",
    "print(\"Avarage Recall for K-NN: {:.2f}%\".format(100 * skip_gram_metrics_avg_kNN[2]/5))\n",
    "print(\"Avarage F1-measure K-NN: SVM {:.2f}%\".format(100 * skip_gram_metrics_avg_kNN[3]/5))\n",
    "print(\"The avarage confusion matrix for K-NN is:\")\n",
    "print(skip_gram_avg_cm_kNN/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bb8dc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Doc2Vec model: \n",
      "Avarage Accuracy for Logistic Regression: 90.33%\n",
      "Avarage Precision for Logistic Regression: 91.17%\n",
      "Avarage Recall for Logistic Regression: 90.83%\n",
      "Avarage F1-measure for Logistic Regression: 90.96%\n",
      "The avarage confusion matrix for Logistic Regression is:\n",
      "[[54.8  7.6  0.8]\n",
      " [ 7.6 78.2  0.4]\n",
      " [ 1.2  0.8 38.8]]\n",
      "Avarage Accuracy for SVM: 92.74%\n",
      "Avarage Precision for SVM: 93.60%\n",
      "Avarage Recall for SVM: 93.27%\n",
      "Avarage F1-measure for SVM: 93.39%\n",
      "The avarage confusion matrix for SVM is:\n",
      "[[55.6  7.2  0.4]\n",
      " [ 5.2 80.6  0.4]\n",
      " [ 0.2  0.4 40.2]]\n",
      "Avarage Accuracy for K-NN: 90.33%\n",
      "Avarage Precision for K-NN: 91.17%\n",
      "Avarage Recall for K-NN: 90.83%\n",
      "Avarage F1-measure K-NN: SVM 90.96%\n",
      "The avarage confusion matrix for K-NN is:\n",
      "[[54.8  7.6  0.8]\n",
      " [ 7.6 78.2  0.4]\n",
      " [ 1.2  0.8 38.8]]\n"
     ]
    }
   ],
   "source": [
    "print(\"For Doc2Vec model: \")\n",
    "print(\"Avarage Accuracy for Logistic Regression: {:.2f}%\".format(100 * doc2vec_metrics_avg_logreg[0]/5))\n",
    "print(\"Avarage Precision for Logistic Regression: {:.2f}%\".format(100 * doc2vec_metrics_avg_logreg[1]/5))\n",
    "print(\"Avarage Recall for Logistic Regression: {:.2f}%\".format(100 * doc2vec_metrics_avg_logreg[2]/5))\n",
    "print(\"Avarage F1-measure for Logistic Regression: {:.2f}%\".format(100 * doc2vec_metrics_avg_logreg[3]/5))\n",
    "print(\"The avarage confusion matrix for Logistic Regression is:\")\n",
    "print(doc2vec_avg_cm_logreg/5)\n",
    "\n",
    "print(\"Avarage Accuracy for SVM: {:.2f}%\".format(100 * doc2vec_metrics_avg_svm[0]/5))\n",
    "print(\"Avarage Precision for SVM: {:.2f}%\".format(100 * doc2vec_metrics_avg_svm[1]/5))\n",
    "print(\"Avarage Recall for SVM: {:.2f}%\".format(100 * doc2vec_metrics_avg_svm[2]/5))\n",
    "print(\"Avarage F1-measure for SVM: {:.2f}%\".format(100 * doc2vec_metrics_avg_svm[3]/5))\n",
    "print(\"The avarage confusion matrix for SVM is:\")\n",
    "print(doc2vec_avg_cm_svm/5)\n",
    "\n",
    "print(\"Avarage Accuracy for K-NN: {:.2f}%\".format(100 * doc2vec_metrics_avg_kNN[0]/5))\n",
    "print(\"Avarage Precision for K-NN: {:.2f}%\".format(100 * doc2vec_metrics_avg_kNN[1]/5))\n",
    "print(\"Avarage Recall for K-NN: {:.2f}%\".format(100 * doc2vec_metrics_avg_kNN[2]/5))\n",
    "print(\"Avarage F1-measure K-NN: SVM {:.2f}%\".format(100 * doc2vec_metrics_avg_kNN[3]/5))\n",
    "print(\"The avarage confusion matrix for K-NN is:\")\n",
    "print(doc2vec_avg_cm_kNN/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe223f",
   "metadata": {},
   "source": [
    "The results show that the TF-IDF model performed the best, followed by CBOW and Doc2Vec, with the Skipgram model performing the worst.The evaluation metrics used for all three models (tf-idf, Skipgram, and Doc2Vec) were accuracy, precision, recall, and F1-score. The average accuracy for all models was above 86%, which indicates that the models are performing well in terms of predicting the correct class.\n",
    "    For the TF-IDF approach, the average accuracy for SVM was 92.11%, with an average precision of 93.11%, recall of 92.38%, and F1-measure of 92.71%. The confusion matrix was similar to that of logistic regression, with some confusion between classes 1 and 2.\n",
    "    For the CBOW model, all three classifiers performed similarly as well, with average accuracy around 91%. The average precision and recall were also around 92%, and the average F1-measure was around 92.5%. The confusion matrix was similar to that of the TF-IDF model.\n",
    "    For the Doc2Vec model, the SVM classifier performed the best, with an average accuracy of 92.74%, while the Logistic Regression classifier performed slightly worse, with an average accuracy of 90.33%. The average precision, recall, and F1-measure were all around 91-93%. The confusion matrix was similar to that of the TF-IDF and CBOW models.\n",
    "    For the Skipgram model, the SVM classifier performed the best, with an average accuracy of 89.59%, followed by the Logistic Regression classifier, with an average accuracy of 86.33%. The average precision, recall, and F1-measure were all around 87-91%. The confusion matrix suggests that there may be some overlap or ambiguity between the classes\n",
    "   Overall SVM perfomed the best for all the above models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd08e8",
   "metadata": {},
   "source": [
    "                                             -Bonus-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3accc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/billyvog/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25edfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64030934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:746: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m lr_clf \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m lr_clf\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39marray(X_train_gmodel), np\u001b[38;5;241m.\u001b[39marray(y_train_gm))\n\u001b[0;32m---> 23\u001b[0m lr_clf_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlr_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_gmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m lr_clf_precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, lr_clf_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     25\u001b[0m lr_clf_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test_gm, lr_clf_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_base.py:425\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m        Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    427\u001b[0m         indices \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/linear_model/_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m    this class would be predicted.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 407\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:770\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m--> 770\u001b[0m             \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExpected 2D array, got 1D array instead:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43marray=\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    771\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReshape your data either using array.reshape(-1, 1) if \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    772\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour data has a single feature or array.reshape(1, -1) \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    773\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mif it contains a single sample.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:1592\u001b[0m, in \u001b[0;36m_array_str_implementation\u001b[0;34m(a, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;66;03m# obtain a scalar and call str on it, avoiding problems for subclasses\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m     \u001b[38;5;66;03m# for which indexing with () returns a 0d instead of a scalar by using\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;66;03m# ndarray's getindex. Also guard against recursive 0d object arrays.\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _guarded_repr_or_str(np\u001b[38;5;241m.\u001b[39mndarray\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(a, ()))\n\u001b[0;32m-> 1592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray2string\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_line_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:736\u001b[0m, in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array2string\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:513\u001b[0m, in \u001b[0;36m_recursive_guard.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:546\u001b[0m, in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# skip over array(\u001b[39;00m\n\u001b[1;32m    544\u001b[0m next_line_prefix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(prefix)\n\u001b[0;32m--> 546\u001b[0m lst \u001b[38;5;241m=\u001b[39m \u001b[43m_formatArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinewidth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnext_line_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43medgeitems\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msummary_insert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlegacy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lst\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:889\u001b[0m, in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, line_width, next_line_prefix, separator, edge_items, summary_insert, legacy)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# invoke the recursive part with an initial index and prefix\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecurser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhanging_indent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_line_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcurr_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# recursive closures have a cyclic reference to themselves, which\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# requires gc to collect (gh-10620). To avoid this problem, for\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# performance and PyPy friendliness, we break the cycle:\u001b[39;00m\n\u001b[1;32m    896\u001b[0m     recurser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:845\u001b[0m, in \u001b[0;36m_formatArray.<locals>.recurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    842\u001b[0m         line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m separator\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trailing_items, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 845\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[43mrecurser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_hanging_indent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m     s, line \u001b[38;5;241m=\u001b[39m _extendLine_pretty(\n\u001b[1;32m    847\u001b[0m         s, line, word, elem_width, hanging_indent, legacy)\n\u001b[1;32m    848\u001b[0m     line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m separator\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:799\u001b[0m, in \u001b[0;36m_formatArray.<locals>.recurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    796\u001b[0m axes_left \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m axis\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axes_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformat_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# when recursing, add a space to align with the [ added, and reduce the\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# length of the line by 1\u001b[39;00m\n\u001b[1;32m    803\u001b[0m next_hanging_indent \u001b[38;5;241m=\u001b[39m hanging_indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:395\u001b[0m, in \u001b[0;36m_object_format\u001b[0;34m(o)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    394\u001b[0m     fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:1488\u001b[0m, in \u001b[0;36m_array_repr_implementation\u001b[0;34m(arr, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1486\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(arr\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m0\u001b[39m,):\n\u001b[0;32m-> 1488\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[43marray2string\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_line_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress_small\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# show zero-length shape unless it is (0,)\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[], shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape),)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:736\u001b[0m, in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array2string\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:513\u001b[0m, in \u001b[0;36m_recursive_guard.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:546\u001b[0m, in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# skip over array(\u001b[39;00m\n\u001b[1;32m    544\u001b[0m next_line_prefix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(prefix)\n\u001b[0;32m--> 546\u001b[0m lst \u001b[38;5;241m=\u001b[39m \u001b[43m_formatArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinewidth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnext_line_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43medgeitems\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msummary_insert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlegacy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lst\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:889\u001b[0m, in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, line_width, next_line_prefix, separator, edge_items, summary_insert, legacy)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# invoke the recursive part with an initial index and prefix\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecurser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhanging_indent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_line_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcurr_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;66;03m# recursive closures have a cyclic reference to themselves, which\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# requires gc to collect (gh-10620). To avoid this problem, for\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# performance and PyPy friendliness, we break the cycle:\u001b[39;00m\n\u001b[1;32m    896\u001b[0m     recurser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:845\u001b[0m, in \u001b[0;36m_formatArray.<locals>.recurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    842\u001b[0m         line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m separator\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trailing_items, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 845\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[43mrecurser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_hanging_indent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m     s, line \u001b[38;5;241m=\u001b[39m _extendLine_pretty(\n\u001b[1;32m    847\u001b[0m         s, line, word, elem_width, hanging_indent, legacy)\n\u001b[1;32m    848\u001b[0m     line \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m separator\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:799\u001b[0m, in \u001b[0;36m_formatArray.<locals>.recurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    796\u001b[0m axes_left \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m axis\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axes_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformat_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# when recursing, add a space to align with the [ added, and reduce the\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# length of the line by 1\u001b[39;00m\n\u001b[1;32m    803\u001b[0m next_hanging_indent \u001b[38;5;241m=\u001b[39m hanging_indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/arrayprint.py:1030\u001b[0m, in \u001b[0;36mFloatingFormat.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_left \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_right \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ret)) \u001b[38;5;241m+\u001b[39m ret\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_format:\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdragon4_scientific\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmin_digits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_digits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                              \u001b[49m\u001b[43munique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtrim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m                              \u001b[49m\u001b[43msign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpad_left\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mexp_digits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dragon4_positional(x,\n\u001b[1;32m   1040\u001b[0m                               precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision,\n\u001b[1;32m   1041\u001b[0m                               min_digits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_digits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1046\u001b[0m                               pad_left\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_left,\n\u001b[1;32m   1047\u001b[0m                               pad_right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_right)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g_model_metrics_avg_logreg = np.zeros(4)\n",
    "g_model_avg_cm_logreg = None\n",
    "\n",
    "\n",
    "for train_index, test_index in kfold.split(word_by_word_X):\n",
    "    X_train, X_test = [word_by_word_X[i] for i in train_index], [word_by_word_X[i] for i in test_index]\n",
    "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "    \n",
    "    train_gmodel = [[g_model[x] for x in y if x in g_model] for y in X_train]\n",
    "    train_data_labels = [(x,y) for (x,y) in zip(train_gmodel, y_train) if len(x) > 0]\n",
    "    X_train_gm = [np.array(x) for (x,y) in train_data_labels]\n",
    "    y_train_gm = [y for (x,y) in train_data_labels]\n",
    "    X_train_gmodel = [x.mean(axis = 0) for x in X_train_gm]\n",
    "    test_gmodel = [[g_model[x] for x in y if x in g_model] for y in X_test]\n",
    "    test_data_labels = [(x,y) for (x,y) in zip(test_gmodel, y_test) if len(x) > 0]\n",
    "    X_test_gm = [np.array(x) for (x,y) in test_data_labels]\n",
    "    y_test_gm = [y for (x,y) in test_data_labels]\n",
    "\n",
    "    X_test_gmodel = [x.mean(axis = 0) for x in X_test_gm]\n",
    "    #classifiers for Doc2Vec model\n",
    "    lr_clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
    "    lr_clf.fit(np.array(X_train_gmodel), np.array(y_train_gm))\n",
    "    lr_clf_pred = lr_clf.predict(test_gmodel)\n",
    "    lr_clf_precision = precision_score(y_test, lr_clf_pred, average='macro') \n",
    "    lr_clf_f1 = f1_score(y_test_gm, lr_clf_pred, average='macro', labels=[0, 1, 2])\n",
    "    lr_clf_accuracy = accuracy_score(y_test_gm, lr_clf_pred)\n",
    "    lr_clf_recall = recall_score(y_test_gm, lr_clf_pred, average='macro')\n",
    "\n",
    "    g_model_metrics_avg_logreg[0] += lr_clf_accuracy\n",
    "    g_model_metrics_avg_logreg[1] += lr_clf_precision\n",
    "    g_model_metrics_avg_logreg[2] += lr_clf_recall\n",
    "    g_model_metrics_avg_logreg[3] += lr_clf_f1\n",
    "    g_model_cm_logreg = confusion_matrix(y_test, lr_clf_pred)\n",
    "    if g_model_avg_cm_logreg is None:\n",
    "        g_model_avg_cm_logreg = g_model_cm_logreg\n",
    "    else:\n",
    "        g_model_avg_cm_logreg += g_model_cm_logreg \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6efc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"For Google word2vec model: \")\n",
    "print(\"Avarage Accuracy for Logistic Regression: {:.2f}%\".format(100 * g_model_metrics_avg_logreg[0]/5))\n",
    "print(\"Avarage Precision for Logistic Regression: {:.2f}%\".format(100 * g_model_metrics_avg_logreg[1]/5))\n",
    "print(\"Avarage Recall for Logistic Regression: {:.2f}%\".format(100 * g_model_metrics_avg_logreg[2]/5))\n",
    "print(\"Avarage F1-measure for Logistic Regression: {:.2f}%\".format(100 * g_model_metrics_avg_logreg[3]/5))\n",
    "print(\"The avarage confusion matrix for Logistic Regression is:\")\n",
    "print(g_model_avg_cm_logreg/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf6177",
   "metadata": {},
   "source": [
    "took forever so i couldnt test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb07c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
